
# ðŸŒ¦ï¸ ETL & Data Analysis Pipeline for Colombian Meteorological Data

This project builds an automated ETL (Extract, Transform, Load) pipeline to extract hydrometeorological data (e.g., maximum daily temperatures) from Colombiaâ€™s IDEAM/DHIME web portal.

It also includes a containerized setup (via Docker), virtual environment support, and advanced data exploration using statistical tests and visualization techniques.


## Project Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/    â† Raw zipped files downloaded from DHIME
â”‚   â”œâ”€â”€ silver/    â† Cleaned CSV files
â”‚   â””â”€â”€ gold/      â† (Optional) Further processed datasets
â”œâ”€â”€ documents/
â”‚   â”œâ”€â”€ rdmp.pdf   â† Data Management Plan 
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_data_analysis.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ orchestrator.py         â† Main pipeline controller
â”‚   â”œâ”€â”€ data_extraction.py      â† Web scraping using Selenium
â”‚   â”œâ”€â”€ preprocessing.py        â† Unzipping and cleaning data
â”‚   â”œâ”€â”€ exploratory_functions.pyâ† Custom EDA/statistics utilities
â”‚   â”œâ”€â”€ chromedriver_config.py  â† Headless Chrome config for Selenium
â”‚   â””â”€â”€ variables.py            â† Dictionary of hydromet variables
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ tests/
    â”œâ”€â”€ test_extraction.py
    â”œâ”€â”€ test_orchestrator.py
    â””â”€â”€ test_preprocessing.py
```


## Pipeline Overview

1. **Extraction** (`data_extraction.py`)
   - Uses Selenium to simulate interaction with the DHIME web platform.
   - Accepts parameters like `variable`, `station code`, `department`, and `date range`.

2. **Transformation** (`preprocessing.py`)
   - Unzips raw `.zip` files into `.csv`.
   - Cleans and filters relevant columns (e.g., `Fecha`, `Valor`).
   - Stores the output in `data/silver/dailymaxtemperature.csv`.

3. **Exploration** (`exploratory_data_analysis.ipynb`)
   - Utilizes advanced statistical diagnostics (KPSS, ADF, Box-Cox, Granger Causality).
   - Visual tools: histograms, seasonal decomposition, ACF/PACF, KDE plots.

4. **Automation** (`orchestrator.py`)
   - Controls the full ETL pipeline in one go.
   - Includes logging, retries, and error handling.


## ðŸ³ Docker Support

**Build:** From father repository
```sh
docker build -f dm_project_docker/Dockerfile -t dm_project_docker:latest dm_project
```

**Run:** from father repository
```sh
docker run -it --rm \
-v "$(pwd)":/workspace \
-w /workspace \
-e PYTHONPATH=/workspace/src \
dm_project_docker:latest \
python3 -m src.orchestrator
```

> Chrome runs in **headless mode** for containers. To visualize browser activity (e.g., debugging), disable headless mode in `src/chromedriver_config.py`.


## ðŸ§ª Testing

**Unit Test Entry Points:**
- `test_extraction.py`
- `test_orchestrator.py`
- `test_preprocessing.py`

**Run tests:**
```sh
pytest -v tests/
```

## Setup Instructions (Local)

### 1. Python & Chrome Requirements

- Python 3.10
- Chrome (v133.0.6943.98 or matching)
- Compatible [ChromeDriver](https://developer.chrome.com/docs/chromedriver/downloads)

### 2. Setup Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

## Running the ETL

Once set up:

```bash
python src/orchestrator.py
```

## Exploratory Data Analysis (EDA)

Open and run the notebook:

```bash
jupyter notebook notebooks/exploratory_data_analysis.ipynb
```

It contains:
- Temporal trends
- Seasonality detection
- Statistical stationarity checks
- Distribution diagnostics
- Granger causality tests

---

## ðŸ“Ž Notes 

- The raw `.zip` file is saved under `data/bronze/`.
- The cleaned `.csv` file is stored under `data/silver/dailymaxtemperature.csv`.
- Additional processed outputs may be stored in `data/gold/` (planned extension).
- The exploratory functions in `exploratory_functions.py` are modular and reusable.

## ðŸ§  Credits & Context

Developed as part of a Data Management course project to explore the integration of hydrometeorological data from open government portals into reproducible scientific workflows. This includes Dockerization, CI compatibility, and advanced data exploration.
